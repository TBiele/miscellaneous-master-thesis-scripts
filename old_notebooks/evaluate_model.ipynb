{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8b158d",
   "metadata": {},
   "source": [
    "This notebook can be used to evaluate the models that were trained on the task of labeling Telegram messages with misconceptions related to COVID-19. It was adapted from a [notebook created by Ronak Patel](https://towardsdatascience.com/transformers-for-multilabel-classification-71a1a0daf5e1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8065cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model to evaluate\n",
    "BASE_MODEL = \"distilbert-base-german-cased\"\n",
    "TRAINED_MODEL_NAME = \"2021-12-20T17-54-59_MULT_DistilBERT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a99b223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f3b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != \"/device:GPU:0\":\n",
    "    raise SystemError(\"GPU device not found\")\n",
    "print(\"Found GPU at: {}\".format(device_name))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6330d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_message(message):\n",
    "    \"\"\"\n",
    "    Process a message by removing German stop words from it. Since the proportion of a \n",
    "    message that can be taken into account by a model is often limited, it might make \n",
    "    sense to do this to consider the more significant words of the message.\n",
    "\n",
    "    Args:\n",
    "        message (str): The message to process\n",
    "\n",
    "    Returns:\n",
    "        str: The processed message\n",
    "    \"\"\"\n",
    "    german_stop_words = nltk.corpus.stopwords.words(\"german\")\n",
    "    input_message_split = message.split()\n",
    "    processed_message = \" \".join(\n",
    "        [w for w in input_message_split if not w in german_stop_words]\n",
    "    )\n",
    "    return processed_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff8c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "\n",
    "# TODO: Get this from model/json file\n",
    "MAX_TOKEN_LENGTH = 280\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "TRAINED_MODEL_DIR = f\"../models/{TRAINED_MODEL_NAME}\"\n",
    "\n",
    "# Load test data\n",
    "df_test = pd.read_csv(f\"{TRAINED_MODEL_DIR}/test.csv\")\n",
    "display(df_test.head())\n",
    "\n",
    "BASE_COLUMNS_COUNT = 2\n",
    "base_columns = list(df_test.columns[:BASE_COLUMNS_COUNT])\n",
    "misconception_columns = list(\n",
    "    df_test.columns[BASE_COLUMNS_COUNT:-1]\n",
    ")  # exclude the last column containing the one-hot vector of labels\n",
    "print(\"Misconceptions:\", misconception_columns)\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    f\"{TRAINED_MODEL_DIR}/model\", num_labels=len(misconception_columns)\n",
    ")\n",
    "model.cuda()\n",
    "model.eval()  # Put model into evaluation mode\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "\n",
    "# Load mappings between labels and label ids\n",
    "id2label = pickle.load(open(f\"{TRAINED_MODEL_DIR}/id2label.p\", \"rb\"))\n",
    "label2id = pickle.load(open(f\"{TRAINED_MODEL_DIR}/label2id.p\", \"rb\"))\n",
    "\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af105b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add one-hot-encoding column\n",
    "df_test[\"one_hot_labels\"] = list(df_test[misconception_columns].astype(\"int\").values)\n",
    "\n",
    "one_hot_labels = list(df_test.one_hot_labels.values)\n",
    "messages_list = list(df_test.content.values)\n",
    "messages_list = [process_message(message) for message in messages_list]\n",
    "\n",
    "# Encode input data\n",
    "encodings = tokenizer.batch_encode_plus(\n",
    "    messages_list, max_length=MAX_TOKEN_LENGTH, padding=True, truncation=True\n",
    ")\n",
    "input_ids = encodings[\"input_ids\"]\n",
    "attention_masks = encodings[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bcc9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tensors out of data\n",
    "inputs = torch.tensor(input_ids)\n",
    "labels = torch.tensor(one_hot_labels)\n",
    "masks = torch.tensor(attention_masks)\n",
    "# Create test dataloader\n",
    "test_data = TensorDataset(inputs, masks, labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "# Save test dataloader\n",
    "# torch.save(test_dataloader,'test_data_loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672889e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track variables\n",
    "logit_preds, true_labels, pred_labels, tokenized_texts = [], [], [], []\n",
    "\n",
    "# Predict\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "        b_logit_pred = outs[0]\n",
    "        pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "        b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "        pred_label = pred_label.to(\"cpu\").numpy()\n",
    "        b_labels = b_labels.to(\"cpu\").numpy()\n",
    "\n",
    "    tokenized_texts.append(b_input_ids)\n",
    "    logit_preds.append(b_logit_pred)\n",
    "    true_labels.append(b_labels)\n",
    "    pred_labels.append(pred_label)\n",
    "\n",
    "# Flatten outputs\n",
    "tokenized_texts = [item for sublist in tokenized_texts for item in sublist]\n",
    "pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "true_labels = [item for sublist in true_labels for item in sublist]\n",
    "# Converting flattened binary values to boolean values\n",
    "true_bools = [tl == 1 for tl in true_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33548780",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bools = [pl > 0.3 for pl in pred_labels]  # boolean output after thresholding\n",
    "\n",
    "# Print and save classification report\n",
    "print(\"F1-Score: \", f1_score(true_bools, pred_bools, average=\"micro\"))\n",
    "print(\"Accuracy: \", accuracy_score(true_bools, pred_bools), \"\\n\")\n",
    "clf_report = classification_report(\n",
    "    true_bools, pred_bools, target_names=misconception_columns\n",
    ")\n",
    "# Save report\n",
    "if not os.path.isdir(f\"{TRAINED_MODEL_DIR}/results/\"):\n",
    "    os.makedirs(f\"{TRAINED_MODEL_DIR}/results/\")\n",
    "with open(f\"{TRAINED_MODEL_DIR}/results/classification_report.txt\", \"w\") as f:\n",
    "    f.write(clf_report)\n",
    "\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37646628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting indices of where boolean one hot vector true_bools is True so we can use \n",
    "# id2label to gather label names\n",
    "true_label_idxs, pred_label_idxs = [], []\n",
    "for vals in true_bools:\n",
    "    true_label_idxs.append(np.where(vals)[0].flatten().tolist())\n",
    "for vals in pred_bools:\n",
    "    pred_label_idxs.append(np.where(vals)[0].flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c361cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering vectors of label names using id2label\n",
    "true_label_texts, pred_label_texts = [], []\n",
    "for vals in true_label_idxs:\n",
    "    if vals:\n",
    "        true_label_texts.append([id2label[val] for val in vals])\n",
    "    else:\n",
    "        true_label_texts.append(vals)\n",
    "\n",
    "for vals in pred_label_idxs:\n",
    "    if vals:\n",
    "        pred_label_texts.append([id2label[val] for val in vals])\n",
    "    else:\n",
    "        pred_label_texts.append(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedebf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding input ids to comment text\n",
    "comment_texts = [\n",
    "    tokenizer.decode(text, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    for text in tokenized_texts\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f7c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting lists to df\n",
    "comparisons_df = pd.DataFrame(\n",
    "    {\n",
    "        \"comment_text\": comment_texts,\n",
    "        \"true_labels\": true_label_texts,\n",
    "        \"pred_labels\": pred_label_texts,\n",
    "    }\n",
    ")\n",
    "comparisons_df.to_csv(\n",
    "    f\"{TRAINED_MODEL_DIR}/results/classification_model_true_predicted_comparison.csv\"\n",
    ")\n",
    "comparisons_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d274274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy - maximize F1 accuracy by tuning threshold values. \n",
    "# First with 'macro_thresholds' on the order of e^-1 \n",
    "# then with 'micro_thresholds' on the order of e^-2\n",
    "\n",
    "macro_thresholds = np.array(range(1, 10)) / 10\n",
    "\n",
    "f1_results, flat_acc_results = [], []\n",
    "for th in macro_thresholds:\n",
    "    pred_bools = [pl > th for pl in pred_labels]\n",
    "    test_f1_accuracy = f1_score(true_bools, pred_bools, average=\"micro\")\n",
    "    test_flat_accuracy = accuracy_score(true_bools, pred_bools)\n",
    "    f1_results.append(test_f1_accuracy)\n",
    "    flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "best_macro_th = macro_thresholds[np.argmax(f1_results)]  # best macro threshold value\n",
    "\n",
    "micro_thresholds = (\n",
    "    np.array(range(10)) / 100\n",
    ") + best_macro_th  # calculating micro threshold values\n",
    "\n",
    "f1_results, flat_acc_results = [], []\n",
    "for th in micro_thresholds:\n",
    "    pred_bools = [pl > th for pl in pred_labels]\n",
    "    test_f1_accuracy = f1_score(true_bools, pred_bools, average=\"micro\")\n",
    "    test_flat_accuracy = accuracy_score(true_bools, pred_bools)\n",
    "    f1_results.append(test_f1_accuracy)\n",
    "    flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "best_f1_idx = np.argmax(f1_results)  # best threshold value\n",
    "\n",
    "# Printing and saving classification report\n",
    "print(\"Best Threshold: \", micro_thresholds[best_f1_idx])\n",
    "print(\"F1-Score: \", f1_results[best_f1_idx])\n",
    "print(\"Accuracy: \", flat_acc_results[best_f1_idx], \"\\n\")\n",
    "\n",
    "best_pred_bools = [pl > micro_thresholds[best_f1_idx] for pl in pred_labels]\n",
    "clf_report_optimized = classification_report(\n",
    "    true_bools, best_pred_bools, target_names=misconception_columns\n",
    ")\n",
    "# Save report\n",
    "if not os.path.isdir(f\"{TRAINED_MODEL_DIR}/results/\"):\n",
    "    os.makedirs(f\"{TRAINED_MODEL_DIR}/results/\")\n",
    "with open(f\"{TRAINED_MODEL_DIR}/results/classification_report_optimized.txt\", \"w\") as f:\n",
    "    f.write(clf_report_optimized)\n",
    "\n",
    "print(clf_report_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcafee86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
